model:
    checkpoint_path: ""
    use_gpu: True
    base_model_type: "saaa"
    base_model_ckpt_path: 
        - "results/clevr/saaa/M0/checkpoints/checkpoint_epoch_015.pkl"
        - "results/clevr/saaa/M1/checkpoints/checkpoint_epoch_016.pkl"
        - "results/clevr/saaa/M2/checkpoints/checkpoint_epoch_013.pkl"
        - "results/clevr/saaa/M3/checkpoints/checkpoint_epoch_016.pkl"
        - "results/clevr/saaa/M4/checkpoints/checkpoint_epoch_015.pkl"
    loss_reduce: False
    vis_individual_net: False
    verbose_all: True
    save_all_net: True
    # question embedding layer
    word_emb_dim: 300
    word_emb_padding_idx: 0
    word_emb_dropout_prob: 0
    apply_word_emb_nonlinear: False
    rnn_type: "LSTM"
    rnn_num_layers: 1
    rnn_hidden_dim: 256
    rnn_dropout_prob: 0
    use_last_hidden: True
    # SAAA layer
    num_stacks: 2
    qst_emb_dim: -1 # will be assigned to answer_mlp_out_dim
    img_emb_dim: 1024 # will be assigned to img_emb2d_out_dim
    att_emb_dim: 512
    att_dropout_prob: 0
    # classification layer
    answer_mlp_inp_dim: -1 # will be assigned automatically
    answer_mlp_out_dim: -1 # will be assigned to num_labels
    answer_mlp_hidden_dim: [1024,]
    answer_mlp_dropout_prob: 0
    answer_mlp_use_batchnorm: False
    answer_mlp_nonlinear_fn: "ReLU"
    # assignment model layer
    use_assignment_model: False
    assignment_img_emb2d_inp_dim: 1024
    assignment_img_emb2d_out_dim: 256
    assignment_img_emb2d_dropout_prob: 0
    assignment_img_emb2d_nonlinear_fn: "None"
    assignment_img_emb2d_use_batchnorm: False
    assignment_img_emb2d_apply_l2_norm: False
    assignment_img_emb2d_only_l2_norm: False
    assignment_fusion_method: "hadamard"
    assignment_fusion_inp1_dim: -1 # will be assigned to qst_emb_dim
    assignment_fusion_inp1_dropout_prob: 0
    assignment_fusion_inp1_nonlinear_fn: "Tanh"
    assignment_fusion_inp2_dim: -1 # will be assigned to img_emb2d_out_dim
    assignment_fusion_inp2_dropout_prob: 0
    assignment_fusion_inp2_nonlinear_fn: "Tanh"
    assignment_fusion_dim: 256
    assignment_mlp_inp_dim: -1 # will be assigned to fusion_dim
    assignment_mlp_out_dim: -1 # will be assigned to num_models
    assignment_mlp_dropout_prob: 0
    assignment_mlp_hidden_dim: [1024,]
    assignment_mlp_use_batchnorm: False
    assignment_mlp_nonlinear_fn: "ReLU"
    # MCL criterion
    version: "CMCL_v1"
    num_models: 5
    num_overlaps: 3
    beta: 0.5
    tau: 1
    use_knowledge_distillation: False
    use_initial_assignment: False
    apply_curriculum_learning_after: -1
    new_loss: "CMCL_v1"
train_loader:
    encoded_json_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_train.json"
    encoded_hdf5_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_train.h5"
    batch_size: 128
    use_gpu: True
    use_img: False
    img_size: 224
    img_dir: "data/CLEVR_v1.0/images"
    feat_dir: "data/CLEVR_v1.0/feats/resnet_conv4_feats"
test_loader:
    encoded_json_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_val.json"
    encoded_hdf5_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_val.h5"
    selection_path: ""
    batch_size: 128
    use_gpu: True
    use_img: False
    img_size: 224
    img_dir: "data/CLEVR_v1.0/images"
    feat_dir: "data/CLEVR_v1.0/feats/resnet_conv4_feats"
optimize:
    num_epoch: 200
    init_lr: 0.0005
    decay_factor: 0.8
    decay_every_epoch: -1
evaluation:
    every_eval: 1
    print_every: 100
logging:
    print_level: "DEBUG"
    write_level: "INFO"
misc:
    print_every: 10
    vis_every: -1
