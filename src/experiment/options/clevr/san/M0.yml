
model:
    checkpoint_path: ""
    use_gpu: True
    loss_reduce: True
    # image embedding layer (for SAN)
    img_emb2d_inp_dim: 1024
    img_emb2d_out_dim: 256
    img_emb2d_dropout_prob: 0
    img_emb2d_nonlinear_fn: "None"
    img_emb2d_apply_l2_norm: False 
    img_emb2d_only_l2_norm: False 
    # question embedding layer
    word_emb_dim: 300
    word_emb_padding_idx: -1 # will be assigned automatically
    word_emb_dropout_prob: 0
    apply_word_emb_nonlinear: False
    rnn_type: "LSTM"
    rnn_num_layers: 2
    rnn_hidden_dim: 256
    rnn_dropout_prob: 0
    use_last_hidden: True
    # SAN (SAAA) layer
    num_stacks: 2
    qst_emb_dim: -1 # will be assigned to answer_mlp_out_dim
    img_emb_dim: -1 # will be assigned to img_emb2d_out_dim
    att_emb_dim: 512
    att_nonlinear_fn: "None"
    # classification layer
    answer_mlp_inp_dim: -1 # will be assigned automatically
    answer_mlp_out_dim: -1 # will be assigned to num_labels
    answer_mlp_hidden_dim: [1024,]
    answer_mlp_dropout_prob: 0
    answer_mlp_use_batchnorm: False
    answer_mlp_nonlinear_fn: "ReLU"
train_loader:
    encoded_json_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_train.json"
    encoded_hdf5_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_train.h5"
    batch_size: 128
    use_gpu: True
    use_img: False
    img_size: 224
    img_dir: "data/CLEVR_v1.0/images"
    feat_dir: "data/CLEVR_v1.0/feats/resnet_conv4_feats"
test_loader:
    encoded_json_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_val.json"
    encoded_hdf5_path: "data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_val.h5"
    selection_path: ""
    batch_size: 128
    use_gpu: True
    use_img: False
    img_size: 224
    img_dir: "data/CLEVR_v1.0/images"
    feat_dir: "data/CLEVR_v1.0/feats/resnet_conv4_feats"
optimize:
    num_epoch: 200
    init_lr: 0.0005
    decay_factor: 0.8
    decay_every_epoch: -1
evaluation:
    every_eval: 1
    print_every: 50
logging:
    print_level: "DEBUG"
    write_level: "INFO"
misc:
    result_dir: "results/cmcl"
    print_every: 100
    vis_every: -1
