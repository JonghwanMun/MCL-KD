# for clevr
batch_size: 64
encoded_hdf5_path: data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_val.h5
encoded_json_path: data/CLEVR_v1.0/preprocess/encoded_qa/vocab_train_raw/all_questions_use_zero_token_max_qst_len_45/qa_val.json
feat_dir: data/CLEVR_v1.0/feats/resnet_conv4_feats
img_dir: data/CLEVR_v1.0/images
img_size: 224
selection_path: ''
use_gpu: true
use_img: false

# for vqa
#batch_size: 128
#encoded_hdf5_path: data/VQA_v2.0/preprocess/encoded_qa/vqa_3000_vocab_train-val_raw/all_questions_with_answer_vocab_use_zero_token_max_qst_len_23/qa_val.h5
#encoded_json_path: data/VQA_v2.0/preprocess/encoded_qa/vqa_3000_vocab_train-val_raw/all_questions_with_answer_vocab_use_zero_token_max_qst_len_23/qa_val.json
#feat_dir: data/VQA_v2.0/feats/resnet_conv4_feats
#feat_type: numpy
#fetching_answer_option: all_answers
#img_dir: data/VQA_v2.0/images
#img_size: 224
#use_gpu: true
#use_img: false
